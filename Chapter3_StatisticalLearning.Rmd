---
title: "Chapter3StatisticalLearning"
author: "Jatinder Singh Malhi"
date: "22/06/2021"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ISLR)
library(MASS)
library(ggeffects)
```
```{r}
d <- data.frame(X1 = c(0,2,0,0,-1,1), X2 = c(3,0,1,1,0,1), X3 = c(0,0,3,2,1,1), Y= c("Red", "Red", "Red","Green","Green","Red"))
d$f <- sqrt((d$X1^2)+(d$X2^2)+(d$X3^2))
arrange(d,d$f)
```
```{r}
head(College)
ggplot(College)+geom_boxplot(mapping = aes(Outstate, fill = "Outstate"),color = "blue")

```

```{r}
College$Elite <- as.factor(College$Top10perc >50)
summary(College)
ggplot(College, mapping = aes(x = Elite,y =Outstate, fill = Elite))+geom_boxplot(outlier.color = "purple",varwidth = TRUE)+ stat_boxplot(geom = 'errorbar')
```

```{r}
par(mfrow = c(2,2))
ggplot(data = College, mapping = aes(Grad.Rate, fill = "Grad.Rate"))+ geom_histogram(bins = 180)
```

```{r}
head(Auto)
ggplot(data = Auto, mapping = aes(x = horsepower, y = mpg))+geom_point(mapping = aes(color = cylinders))+geom_smooth()
```

```{r}
head(Boston)
ggplot(data = Boston, mapping = aes(x  =  lstat, y = medv))+geom_point(mapping = aes(color = rm)) +geom_smooth()
ggplot(data = Boston,mapping = aes(tax, fill = "tax"))+ geom_boxplot()+stat_boxplot(geom = 'errorbar') + coord_flip()
```
```{r}
head(Credit)
Credit <- Credit[,c("Income", "Balance", "Student")]
Credit$Student <- factor(as.numeric(Credit$Student == "Yes"))
ggplot(data = Credit, aes(Income, Balance, color = Student, fill = Student))+geom_point(size = 2)+stat_smooth(method = "lm")
```
```{r}
head(Boston)
lm_linear <- lm(medv ~ lstat, data = Boston)
summary(lm_linear)
coefficients(lm_linear)
plot(Boston$lstat, Boston$medv, col = "green")
abline(lm_linear, col = "blue")
par(mfrow = c(2,2))
plot(lm_linear)
quad_model <- lm(medv ~ lstat + I(lstat^2), data = Boston)
par(mfrow = c(2,2))
plot(quad_model)
anova(lm_linear, quad_model)
```

```{r}
head(Carseats)
lm_interaction <- lm(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)
#ggpredict(lm_interaction , interactive = TRUE)
par(mfrow = c(2,2))
plot(lm_interaction)
```

```{r}
head(Auto)
ggplot(data = Auto, mapping = aes(x = horsepower, y = mpg)) + geom_point()+ stat_smooth(method = "lm")
auto_linear <- lm(data = Auto, mpg ~ horsepower)
ggpredict(auto_linear, interactive = TRUE)
summary(auto_linear)
par(mfrow = c(2,2))
plot(auto_linear)

new_horsepower <- data.frame(horsepower = c(98))
predict(auto_linear, new_data = new_horsepower,interval = "confidence")
predict(auto_linear, newdata = new_horsepower,interval = "prediction")
```
```{r}
head(Auto)
plot(Auto[,1:6], col = "blue")
cor(Auto[,1:6])
multi_linearauto <- lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + year + origin, data = Auto)
summary(multi_linearauto)
par(mfrow = c(2,2))
plot(multi_linearauto)
```

```{r}
head(Carseats)
car_multilinear <- lm(Sales ~ Price + Urban + US , data = Carseats)
car_linear <- lm(Sales ~ Price  , data = Carseats)
plot(Carseats$Price, Carseats$Sales)
abline(car_linear)
par(mfrow = c(2,2))
plot(car_linear)
plot(car_multilinear)
summary(car_multilinear)
summary(car_linear)
ggpredict(car_linear)
```

```{r}
set.seed(1)
x = rnorm(100)
y = 2*x + rnorm(100)
withoiutInterceptLinear <- lm(y ~ x + 0)
xOntoY <- lm(x ~ y + 0)
summary(withoiutInterceptLinear)
summary(xOntoY)
```

Exercise 12
```{r}
set.seed(1)
X = rnorm(100, mean = 1000, sd = 0.1)
Y = rnorm(100, mean = 1000, sd = 0.1)
yOntox <- lm(Y ~ X )
summary(yOntox)
plot(X, Y)
abline(yOntox)
par(mfrow = c(2,2))
plot(yOntox)

xOntoy <- lm(X ~ Y)
summary(xOntoy)
plot(Y, X)
abline(xOntoy)
par(mfrow = c(2,2))
plot(xOntoy)
```

Exercise13
```{r}

X = rnorm(100, mean = 0, sd = 1)
length(X)
eps = rnorm(100, mean = 0, sd = 0.25)
Y = -1 + 0.5 * X + eps
length(Y)
least_model <- lm(Y ~ X)
plot(X, Y)
abline(-1, 0.5, col = "blue")
abline(least_model, col = "red")
summary(least_model)
legend("topleft",
       legend = c("Line1", "Line2"),
       col = c("blue", "red"), lwd = 3)

ggplot(mapping = aes(x = X, y = Y)) + geom_point() + stat_smooth(method = "lm" ,
                                                                 formula = y ~ poly(x, 2), size = 1, color = "blue") + stat_smooth(method = "lm" , formula = y ~ x, size = 1, color = "red")
quad_model <- lm(Y ~ poly(X,2))
anova(least_model, quad_model)
ggpredict(quad_model)
ggpredict(least_model)
```

Exercise14
```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5*x1 + rnorm(100)/10
y <- 2 + 2*x1 + 0.3*x2 + rnorm(100)
forteen_linear <- lm(y ~ x1 + x2)
cor(x1,x2)
ggplot() + geom_jitter(mapping = aes(x = x1, y = y), color = "green") + stat_smooth(method = "lm" ,aes(x = x1, y = y), se = FALSE) +
  geom_jitter(mapping = aes(x = x2, y = y), color = "red") + stat_smooth(method = "lm" ,aes(x = x2, y = y), se = FALSE)
summary(forteen_linear)
```


















